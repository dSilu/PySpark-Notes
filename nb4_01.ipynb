{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://fedora:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SQL-use</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f86dfa881f0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SQL-use\").getOrCreate()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['filamentA', '100W', 605],\n",
       " ['filamentB', '100W', 683],\n",
       " ['filamentB', '100W', 691],\n",
       " ['filamentB', '200W', 561]]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame\n",
    "\n",
    "filament_data = [ \n",
    "    [\"filamentA\", '100W', 605],\n",
    "    [\"filamentB\", '100W', 683],\n",
    "    [\"filamentB\", '100W', 691],\n",
    "    [\"filamentB\", '200W', 561],\n",
    "    [\"filamentA\", '200W', 530],\n",
    "    [\"filamentA\", '100W', 619],\n",
    "    ['filamentB', '100W', 686],\n",
    "    ['filamentB', '200W', 600],\n",
    "    ['filamentB', '100W', 696],\n",
    "    ['filamentA', '200W', 579],\n",
    "    ['filamentA', '200W', 520],\n",
    "    ['filamentA', '100W', 622],\n",
    "    ['filamentA', '100W', 668],\n",
    "    ['filamentB', '200W', 569],\n",
    "    ['filamentB', '200W', 555],\n",
    "    ['filamentA', '200W', 541]\n",
    "]\n",
    "\n",
    "\n",
    "filament_rdd = spark.sparkContext.parallelize(filament_data, 4)\n",
    "filament_rdd.take(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating DataFrame\n",
    "\n",
    "- A DataFrame can be created from an RDD or by reading a file.\n",
    "\n",
    "- `SQLContext` is the entering point to the PySparkSQL.\n",
    "\n",
    "- Sometimes we need to create a schema with `StructType`.\n",
    "\n",
    "- `StructType` takes a list of column names that are defined by `StructField`.\n",
    "\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import * # to import HQL data types\n",
    "from pyspark.sql import Row # to create a row object\n",
    "from pyspark.sql import SQLContext # SQLContext object is the entry point to PySparkSQL.\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`StructField(name: str, dataType: DataType, nullable: bool=True, metadata:dict)`**\n",
    "\n",
    "- name: name of the column\n",
    "\n",
    "- dataType: DataType of the field\n",
    "\n",
    "- nullable: whether the column can have null value or not\n",
    "\n",
    "- metadata (optional)\n",
    "\n",
    "- use to create a column of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('FilamentType', StringType(), True), StructField('BulbPower', StringType(), True), StructField('LifeInHours', StringType(), True)])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define columns\n",
    "col_filament_type = StructField(\"FilamentType\", StringType(), True)\n",
    "col_bulb_power = StructField(\"BulbPower\", StringType(), True)\n",
    "col_life_in_hours = StructField(\"LifeInHours\", StringType(), True)\n",
    "\n",
    "# define the schema\n",
    "filament_data_schema = StructType([col_filament_type, col_bulb_power, col_life_in_hours])\n",
    "\n",
    "filament_data_schema\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To create a row object, Row object is required.\n",
    "\n",
    "`from pyspark.sql import Row`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<Row('filamentA', '100W', '605')>,\n",
       " <Row('filamentB', '100W', '683')>,\n",
       " <Row('filamentB', '100W', '691')>,\n",
       " <Row('filamentB', '200W', '561')>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create row object\n",
    "filament_rdd_rows = filament_rdd.map(lambda x: Row(str(x[0]), str(x[1]), str(x[2])))\n",
    "filament_rdd_rows.take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----------+\n",
      "|FilamentType|BulbPower|LifeInHours|\n",
      "+------------+---------+-----------+\n",
      "|   filamentA|     100W|        605|\n",
      "|   filamentB|     100W|        683|\n",
      "|   filamentB|     100W|        691|\n",
      "|   filamentB|     200W|        561|\n",
      "+------------+---------+-----------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create sql context\n",
    "\n",
    "sqlcontext = SQLContext(spark.sparkContext)\n",
    "\n",
    "filament_df_raw = sqlcontext.createDataFrame(filament_rdd_rows, filament_data_schema)\n",
    "\n",
    "filament_df_raw.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FilamentType: string (nullable = true)\n",
      " |-- BulbPower: string (nullable = true)\n",
      " |-- LifeInHours: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filament_df_raw.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Changing Data Type of a Column**\n",
    "\n",
    "- To change the data type of a column we need the `cast` function.\n",
    "\n",
    "- `cast` function works with `withColumn`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FilamentType: string (nullable = true)\n",
      " |-- BulbPower: string (nullable = true)\n",
      " |-- LifeInHours: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# the above DataFrame, column LifeInHours column has a string data type. \n",
    "# Let's change the data type to float \n",
    "\n",
    "filament_df = filament_df_raw.withColumn(\"LifeInHours\", filament_df_raw.LifeInHours.cast(FloatType()))\n",
    "\n",
    "filament_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----------+\n",
      "|FilamentType|BulbPower|LifeInHours|\n",
      "+------------+---------+-----------+\n",
      "|   filamentA|     100W|      605.0|\n",
      "|   filamentB|     100W|      683.0|\n",
      "|   filamentB|     100W|      691.0|\n",
      "|   filamentB|     200W|      561.0|\n",
      "|   filamentA|     200W|      530.0|\n",
      "|   filamentA|     100W|      619.0|\n",
      "|   filamentB|     100W|      686.0|\n",
      "|   filamentB|     200W|      600.0|\n",
      "|   filamentB|     100W|      696.0|\n",
      "|   filamentA|     200W|      579.0|\n",
      "|   filamentA|     200W|      520.0|\n",
      "|   filamentA|     100W|      622.0|\n",
      "|   filamentA|     100W|      668.0|\n",
      "|   filamentB|     200W|      569.0|\n",
      "|   filamentB|     200W|      555.0|\n",
      "|   filamentA|     200W|      541.0|\n",
      "+------------+---------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filament_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----------+\n",
      "|FilamentType|BulbPower|LifeInHours|\n",
      "+------------+---------+-----------+\n",
      "|   filamentA|     100W|      605.0|\n",
      "|   filamentB|     100W|      683.0|\n",
      "|   filamentB|     100W|      691.0|\n",
      "|   filamentA|     100W|      619.0|\n",
      "|   filamentB|     100W|      686.0|\n",
      "|   filamentB|     100W|      696.0|\n",
      "|   filamentA|     100W|      622.0|\n",
      "|   filamentA|     100W|      668.0|\n",
      "+------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filter out data where bulb power is 100W\n",
    "\n",
    "filament_100w_df = filament_df.filter(filament_df.BulbPower == \"100W\")\n",
    "filament_100w_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----------+\n",
      "|FilamentType|BulbPower|LifeInHours|\n",
      "+------------+---------+-----------+\n",
      "|   filamentB|     100W|      683.0|\n",
      "|   filamentB|     100W|      691.0|\n",
      "|   filamentB|     100W|      686.0|\n",
      "|   filamentB|     100W|      696.0|\n",
      "|   filamentA|     100W|      668.0|\n",
      "+------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filament_100w_650_plus_df = filament_df.filter((filament_df.BulbPower == \"100W\") & (filament_df.LifeInHours > 650.0))\n",
    "filament_100w_650_plus_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_ttl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Dec  7 2022, 00:00:00) [GCC 12.2.1 20221121 (Red Hat 12.2.1-4)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e5fe3c56a0f8f95c221320b0bd7b50af4bf76cfb19c727317c7f47c8ed217d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
