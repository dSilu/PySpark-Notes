{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RDD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RDD: Resilient Distributed Dataset\n",
    "\n",
    "- Main components of the Spark architecture are the _driver_ and _executor_.\n",
    "\n",
    "- There will be one driver program and one or more executors run on the cluster slave machine.\n",
    "\n",
    "- Spark follows master-slave architecture.\n",
    "\n",
    "- PySpark is dispatched with Standalone Cluster Manager. \n",
    "\n",
    "- PySpark can also be configured on YARN and Apache Mesos.\n",
    "\n",
    "\n",
    "### Driver:\n",
    "\n",
    "- Coordinates with many executors running on various slave machines.\n",
    "\n",
    "- _SparkContext_ object is created by the driver, which is the main entry point to a PySpark application.\n",
    "\n",
    "- driver breaks our application into small tasks; a task is the smallest unit of your application.\n",
    "\n",
    "- Tasks are run on different executors in parallel. \n",
    "\n",
    "- The driver is also responsible for scheduling tasks to different executors.\n",
    "\n",
    "### Executors:\n",
    "\n",
    "- Slave processes.\n",
    "\n",
    "- Runs tasks.\n",
    "\n",
    "- Has the capability to cache data in memory by using the BlockManager process.\n",
    "\n",
    "- Each executor runs in its own Java Virtual Machine (JVM)\n",
    "\n",
    "![](spark-architecture.png)\n",
    "\n",
    "### RDD:\n",
    "\n",
    "- RDD is a dataset abstraction over the distributed collection.\n",
    "\n",
    "- An RDD is recomputed on node failures. \n",
    "\n",
    "- Only part of the data is calculated or recalculated as required.\n",
    "\n",
    "- RDDs are immutable\n",
    "\n",
    "- Can perform two types of operations on RDD:\n",
    "    1. Transformation\n",
    "    2. Action\n",
    "\n",
    "- Transformation on an RDD returns another RDD.\n",
    "\n",
    "- Transformations are lazy, actions are eagerly evaluated.\n",
    "\n",
    "- All the transformations are applied when the first action is called, that's why transformation are lazy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/25 00:03:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# import pyspark libraries\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TTY\").getOrCreate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`parallelize(c, numSlices)`:\n",
    "\n",
    "- `parallelize()` function is used to parallelize or distribute the data.\n",
    "\n",
    "- `c`: collection\n",
    "\n",
    "- `numSlices`: number of distributed chunks of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create an RDD\n",
    "l = [i for i in range(1,22,2)]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(l, 2)\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first() # takes first element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# num partitions \n",
    "\n",
    "rdd.getNumPartitions()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Convert Temperature Data**\n",
    "\n",
    "$$C{\\degree} = (F - 32) * 5/9$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[59, 57.2, 53.6, 55.4, 51.8, 53.6, 55.4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = [59, 57.2, 53.6, 55.4, 51.8, 53.6, 55.4]\n",
    "temp_rdd = spark.sparkContext.parallelize(temp_data, 2)\n",
    "temp_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.0, 14.000000000000002, 12.0, 13.0, 10.999999999999998, 12.0, 13.0]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fahrenheitToCentigrade(t):\n",
    "    \"\"\"Convert Fahrenheit to Centigrade\"\"\"\n",
    "    return (t-32) * 5/9\n",
    "\n",
    "temp_centigrade_rdd = temp_rdd.map(fahrenheitToCentigrade)\n",
    "temp_centigrade_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15.0, 14.000000000000002, 13.0, 13.0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filter temp. > than 13 C\n",
    "\n",
    "filtered_temp_rdd = temp_centigrade_rdd.filter(lambda x: x >= 13)\n",
    "filtered_temp_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['a', 'b', 2, 3], ['i', 'j', 1, 4], ['q', 'r', 6, 8]]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rdd1 = rdd.map(lambda x: [x[0], x[1], int(x[2]), int(x[3])])\n",
    "rdd1.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Basic Manipulation**\n",
    "\n",
    "Manipulation on student grades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si1', 'year1', 62.08, 62.4],\n",
       " ['si1', 'year2', 75.94, 76.75],\n",
       " ['si2', 'year1', 68.26, 72.95],\n",
       " ['si2', 'year2', 85.49, 75.8],\n",
       " ['si3', 'year1', 75.08, 79.84],\n",
       " ['si3', 'year2', 54.98, 87.72],\n",
       " ['si4', 'year1', 50.03, 66.85],\n",
       " ['si4', 'year2', 71.26, 69.77],\n",
       " ['si5', 'year1', 52.74, 76.27],\n",
       " ['si5', 'year2', 50.39, 68.58],\n",
       " ['si6', 'year1', 74.86, 60.8],\n",
       " ['si6', 'year2', 58.29, 62.38],\n",
       " ['si7', 'year1', 63.95, 74.51],\n",
       " ['si7', 'year2', 66.69, 56.92]]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "students_mark_sheet = [\n",
    "    [\"si1\", \"year1\", 62.08, 62.4],\n",
    "    [\"si1\", \"year2\", 75.94, 76.75],\n",
    "    ['si2', 'year1', 68.26, 72.95],\n",
    "    ['si2', 'year2', 85.49, 75.8],\n",
    "    ['si3', 'year1', 75.08, 79.84],\n",
    "    ['si3', 'year2', 54.98, 87.72],\n",
    "    ['si4', 'year1', 50.03, 66.85],\n",
    "    ['si4', 'year2', 71.26, 69.77],\n",
    "    ['si5', 'year1', 52.74, 76.27],\n",
    "    ['si5', 'year2', 50.39, 68.58],\n",
    "    ['si6', 'year1', 74.86, 60.8],\n",
    "    ['si6', 'year2', 58.29, 62.38],\n",
    "    ['si7', 'year1', 63.95, 74.51],\n",
    "    ['si7', 'year2', 66.69, 56.92]\n",
    "]\n",
    "\n",
    "mark_sheet_rdd = spark.sparkContext.parallelize(students_mark_sheet)\n",
    "mark_sheet_rdd.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`collect()`**:\n",
    "\n",
    "- `collect` func takes the whole RDD to the driver.\n",
    "\n",
    "- If size of the RDD is very large, the driver may face a memory issue.\n",
    "\n",
    "- `take(n)` fetches first n elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si1', 'year1', 62.08, 62.4],\n",
       " ['si1', 'year2', 75.94, 76.75],\n",
       " ['si2', 'year1', 68.26, 72.95]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mark_sheet_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si1', 'year1', 62.239999999999995],\n",
       " ['si1', 'year2', 76.345],\n",
       " ['si2', 'year1', 70.605],\n",
       " ['si2', 'year2', 80.645],\n",
       " ['si3', 'year1', 77.46000000000001],\n",
       " ['si3', 'year2', 71.35],\n",
       " ['si4', 'year1', 58.44],\n",
       " ['si4', 'year2', 70.515],\n",
       " ['si5', 'year1', 64.505],\n",
       " ['si5', 'year2', 59.485],\n",
       " ['si6', 'year1', 67.83],\n",
       " ['si6', 'year2', 60.335],\n",
       " ['si7', 'year1', 69.23],\n",
       " ['si7', 'year2', 61.805]]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average semester grades\n",
    "\n",
    "avg_marks = mark_sheet_rdd.map(lambda x: [x[0], x[1], (x[2] + x[3])/2])\n",
    "avg_marks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si1', 'year2', 76.345],\n",
       " ['si2', 'year2', 80.645],\n",
       " ['si3', 'year2', 71.35],\n",
       " ['si4', 'year2', 70.515],\n",
       " ['si5', 'year2', 59.485],\n",
       " ['si6', 'year2', 60.335],\n",
       " ['si7', 'year2', 61.805]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# student's average grade in second year\n",
    "second_year_marks = avg_marks.filter(lambda x: \"year2\" in x)\n",
    "second_year_marks.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['si2', 'year2', 80.645],\n",
       " ['si1', 'year2', 76.345],\n",
       " ['si3', 'year2', 71.35],\n",
       " ['si4', 'year2', 70.515],\n",
       " ['si7', 'year2', 61.805],\n",
       " ['si6', 'year2', 60.335],\n",
       " ['si5', 'year2', 59.485]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top three students\n",
    "\n",
    "sorted_marks_desc = second_year_marks.sortBy(keyfunc= lambda x: -x[2]) # descending order\n",
    "sorted_marks_desc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si5', 'year2', 59.485],\n",
       " ['si6', 'year2', 60.335],\n",
       " ['si7', 'year2', 61.805],\n",
       " ['si4', 'year2', 70.515],\n",
       " ['si3', 'year2', 71.35],\n",
       " ['si1', 'year2', 76.345],\n",
       " ['si2', 'year2', 80.645]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_marks_asc = second_year_marks.sortBy(keyfunc= lambda x: x[2]) # ascending order\n",
    "sorted_marks_asc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si2', 'year2', 80.645], ['si1', 'year2', 76.345], ['si3', 'year2', 71.35]]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top 3 students\n",
    "sorted_marks_desc.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si2', 'year2', 80.645], ['si1', 'year2', 76.345], ['si3', 'year2', 71.35]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# takeOrdered func\n",
    "top_three_students = second_year_marks.takeOrdered(num=3, key=lambda x: -x[2])\n",
    "top_three_students"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`takeOrdered(num, key)`**:\n",
    "\n",
    "- `takeOrdered` is an action\n",
    "\n",
    "- num: number of records to choose\n",
    "\n",
    "- key: a list or a function that results sorted result\n",
    "\n",
    "- returns a list, as an action it does not require `collect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si5', 'year2', 59.485], ['si6', 'year2', 60.335], ['si7', 'year2', 61.805]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bottom three students in second year\n",
    "\n",
    "bottom_three_students = second_year_marks.takeOrdered(num=3, key=lambda x: x[2])\n",
    "bottom_three_students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['si2', 'year2', 80.645]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# students with 80% averages\n",
    "\n",
    "more_than_80 = second_year_marks.filter(lambda x: x[2] > 80)\n",
    "more_than_80.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set Operations on RDDs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2001 = \"RIN1 RIN2 RIN3 RIN4 RIN5 RIN6 RIN7\".split() # projects in year 2001\n",
    "data2002 = \"RIN3 RIN4 RIN7 RIN8 RIN9\".split() # projects in year 2002\n",
    "data2003 = \"RIN4 RIN8 RIN10 RIN11 RIN12\".split() # projects in year 2003\n",
    "\n",
    "# create RDDs\n",
    "rdd2001 = spark.sparkContext.parallelize(data2001,2)\n",
    "rdd2002 = spark.sparkContext.parallelize(data2002, 3)\n",
    "rdd2003 = spark.sparkContext.parallelize(data2003, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RIN1',\n",
       " 'RIN2',\n",
       " 'RIN3',\n",
       " 'RIN4',\n",
       " 'RIN5',\n",
       " 'RIN6',\n",
       " 'RIN7',\n",
       " 'RIN3',\n",
       " 'RIN4',\n",
       " 'RIN7',\n",
       " 'RIN8',\n",
       " 'RIN9',\n",
       " 'RIN4',\n",
       " 'RIN8',\n",
       " 'RIN10',\n",
       " 'RIN11',\n",
       " 'RIN12']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# projects initiated in three years\n",
    "\n",
    "all_projects = rdd2001.union(rdd2002).union(rdd2003)\n",
    "all_projects.collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set operation**\n",
    "\n",
    "- In PySpark set operations are *pseudo set operations* because the operations give duplicated result.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_projects.distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RIN7',\n",
       " 'RIN12',\n",
       " 'RIN2',\n",
       " 'RIN6',\n",
       " 'RIN10',\n",
       " 'RIN11',\n",
       " 'RIN9',\n",
       " 'RIN5',\n",
       " 'RIN1',\n",
       " 'RIN3',\n",
       " 'RIN4',\n",
       " 'RIN8']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or\n",
    "all_projects = rdd2001.union(rdd2002).union(rdd2003).distinct()\n",
    "all_projects.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RIN2', 'RIN5', 'RIN1', 'RIN6']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# project completed in first year : first_year.subtract(second_year)\n",
    "first_year_completion = rdd2001.subtract(rdd2002)\n",
    "first_year_completion.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RIN7', 'RIN2', 'RIN6', 'RIN9', 'RIN5', 'RIN1', 'RIN3']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# projects completed in the first two years\n",
    "first_second_year_completion = rdd2001.union(rdd2002).subtract(rdd2003).distinct()\n",
    "first_second_year_completion.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RIN7', 'RIN3']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# projects started in 2001 and continued through 2003\n",
    "\n",
    "rdd2001.intersection(rdd2002).subtract(rdd2003).distinct().collect()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Summary Statistics**\n",
    "\n",
    "- `count()`: number of elements\n",
    "\n",
    "- Sum: There are two ways to find the sum; first `sum()` then `reduce()`.\n",
    "\n",
    "- Mean: Center point of the given data; `mean()`, `fold()`\n",
    "\n",
    "$$mean = \\frac{\\sum\\limits_{i=1}^n x_i}{n}$$\n",
    "\n",
    "- Variance: Indicates the spread of the data around the mean; `variance()`, `sampleVariance()` to calculate sample variance.\n",
    "\n",
    "$$variance = \\frac{\\sum\\limits_{i=1}^n (x_i - mean)^{2}}{n}$$\n",
    "$$sample variance = \\frac{\\sum\\limits_{i=1}^n (x_i - mean)^{2}}{n-1}$$\n",
    "- Standard Deviation: `stdev()` and `sampleStdev()`.\n",
    "\n",
    "- Alternatively we can use `stats()` to find summary statistics of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 13, 15, 12, 11, 12, 11]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air_velocity_kmph = [12, 13, 15, 12, 11, 12, 11]\n",
    "air_velocity_rdd = spark.sparkContext.parallelize(air_velocity_kmph)\n",
    "air_velocity_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of data points\n",
    "air_velocity_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Air velocity in a day: sum\n",
    "air_velocity_rdd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.285714285714286\n",
      "12.285714285714286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# mean air velocity:\n",
    "print(air_velocity_rdd.sum()/air_velocity_rdd.count())\n",
    "\n",
    "print(air_velocity_rdd.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.63265306122449"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# variance \n",
    "air_velocity_rdd.variance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.904761904761905"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample variance\n",
    "\n",
    "air_velocity_rdd.sampleVariance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2777531299998799"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# standard deviation\n",
    "air_velocity_rdd.stdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3801311186847085"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample standard deviation\n",
    "air_velocity_rdd.sampleStdev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count: \t7\n",
      "mean: \t12.285714285714286\n",
      "sum: \t86.0\n",
      "min: \t11.0\n",
      "max: \t15.0\n",
      "stdev: \t1.3801311186847085\n",
      "variance: \t1.904761904761905\n"
     ]
    }
   ],
   "source": [
    "# stats\n",
    "for k in air_velocity_rdd.stats().asDict():\n",
    "    print(\"{}: \\t{}\".format(k,air_velocity_rdd.stats().asDict()[k]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`stats()`**:\n",
    "\n",
    "- Returns a tuple like structure with key value paired called `startCounter`.\n",
    "\n",
    "- Individual elements can be fetched from the stats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 12.285714285714286\n",
      "variance: 1.63265306122449\n",
      "st. deviation: 1.2777531299998799\n",
      "count: 7\n",
      "min: 11\n"
     ]
    }
   ],
   "source": [
    "print(\"mean: {}\".format(air_velocity_rdd.stats().mean()))\n",
    "print(\"variance: {}\".format(air_velocity_rdd.stats().variance()))\n",
    "print(\"st. deviation: {}\".format(air_velocity_rdd.stats().stdev()))\n",
    "print(\"count: {}\".format(air_velocity_rdd.stats().count()))\n",
    "print(\"min: {}\".format(air_velocity_rdd.min()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spark_ttl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e5fe3c56a0f8f95c221320b0bd7b50af4bf76cfb19c727317c7f47c8ed217d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
